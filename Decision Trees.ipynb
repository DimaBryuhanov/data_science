{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desision Trees\n",
    "\n",
    "### Types of nodes:\n",
    "1. Root node (the base of the tree)\n",
    "2. Desision nodes (splitting or desision nodes)\n",
    "3. leaf nodes (end nodes)\n",
    "\n",
    "### Works well with:\n",
    "1. Continuous input variables\n",
    "2. Categorical variables\n",
    "\n",
    "### Benefits of using:\n",
    "1. Nonlinear models\n",
    "2. Easy to interpret (close to human thinking)\n",
    "3. Can be graphically represented\n",
    "4. Require less data preparation\n",
    "\n",
    "### Assumptions:\n",
    "1. The root node comprises the entire training set\n",
    "2. Predictive features should be categorical or if continuous should be binned prior to model deployment\n",
    "3. Rows in the dataset have a recursive distribution based on the values of attributes\n",
    "\n",
    "### Flow of work\n",
    "1. Deploy recursive binary splitting to stratify or segment the predictor space into a number of simple, nonoverlapping regions\n",
    "2. Make a prediction for a given observation by using the mean or the mode of the training observations in the region to which it belongs.\n",
    "3. Use the set of splitting rules to summurize the tree.\n",
    "\n",
    "### Splitting:\n",
    "1. In *regression* trees: SSE is used to calculate loss & thus find the best split\n",
    "2. In *classification* trees: Gini is used for the same (Gini = measure of total variance accross the K classes, it measures the probability of misclassifiation)\n",
    "\n",
    "### Disadvantages:\n",
    "1. Non robust\n",
    "2. Sensitive to training data\n",
    "3. Global optimum of the tree is not guaranteed\n",
    "\n",
    "This means: small changes in the input data can cause large changes in the output.\n",
    "\n",
    "### When to use:\n",
    "1. *Regression* is used for continuous, linerly dependant featers & targets. Output gives mean response used for prediction\n",
    "2. *Classification* is used when the target is  binary categorical variable\n",
    "\n",
    "*Tree pruning* is the process used to overcome model overfitting by removing subnodes of a desision tree (=replacing a whole subtree by a leaf node). Why, because, a tree grown deep will probably overfit on training data and will show bad performance on the unseen data.\n",
    "\n",
    "### Popular tree prunning methods:\n",
    "1. Hold-out-test\n",
    "2. Cost-complexity prunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
